<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NeurIPS on Cloud architect insights :: Joshua Mangold</title>
    <link>https://mangoldai.com/tags/neurips/</link>
    <description>Recent content in NeurIPS on Cloud architect insights :: Joshua Mangold</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 18 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://mangoldai.com/tags/neurips/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Unified Approach to Interpreting Model Predictions with SHAP</title>
      <link>https://mangoldai.com/posts/unified_approach_to_interpreting_model_predictions_with_shap/</link>
      <pubDate>Sat, 18 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mangoldai.com/posts/unified_approach_to_interpreting_model_predictions_with_shap/</guid>
      <description>Unified Approach to Interpreting Model Predictions with SHAP In the era of complex and highly accurate machine learning models, a critical challenge has emerged - the tension between model accuracy and interpretability. While advanced models like ensemble methods and deep neural networks can achieve state-of-the-art performance on large, modern datasets, their inner workings often remain opaque, making it difficult for even experts to understand why they make certain predictions.
To address this issue, researchers Scott M.</description>
    </item>
    
  </channel>
</rss>
